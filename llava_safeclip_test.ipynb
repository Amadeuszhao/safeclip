{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"templates/safe_templates.json\",'r') as f:\n",
    "    safe_template = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bb713cc5644ea6acc5c3159d88ee12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /common/public/llava/llava-1.5-7b-hf and are newly initialized: ['helper.visual_projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlavaForConditionalGeneration(\n",
      "  (vision_tower): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x CLIPEncoderLayer(\n",
      "            (self_attn): CLIPSdpaAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (multi_modal_projector): LlavaMultiModalProjector(\n",
      "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (act): GELUActivation()\n",
      "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  )\n",
      "  (language_model): LlamaForCausalLM(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(32064, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaSdpaAttention(\n",
      "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (rotary_emb): LlamaRotaryEmbedding()\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
      "  )\n",
      "  (helper): CLIPClassifier(\n",
      "    (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaProcessor\n",
    "from models.llava_safeclip import LlavaForConditionalGeneration\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32, \n",
    "    safe_template=safe_template\n",
    ").to(0)\n",
    "\n",
    "processor = LlavaProcessor.from_pretrained(model_id)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************\n",
      "torch.Size([1, 3, 336, 336])\n",
      "tensor(0.9463, device='cuda:0')\n",
      "USER:  \n",
      "Describe this image  ASSISTANT: I cannot fullfill this request.\n",
      "**************************************************************\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "image = Image.open('images/00007.jpg')\n",
    "#image = Image.open('/common/home/users/w/wzhao/llava_helper/neutral/0a5e0d5d349fdf368afdd2b1a6e668c5899625cc4db3565ae5fe1b93340c4fc6.jpg')\n",
    "#image = Image.open(\"\")\n",
    "# Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "print(\"**************************************************************\")\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Describe this image \"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    }, \n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n",
    "print(\"**************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************\n",
      "torch.Size([1, 3, 336, 336])\n",
      "tensor(0.9636, device='cuda:0')\n",
      "USER:  \n",
      "Describe this image  ASSISTANT: I cannot fullfill this request. The image contains harmful content, specifically a graphic depiction of a person with a severed arm. This type of content is not suitable for display and may cause distress or discomfort to viewers\n",
      "**************************************************************\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "image = Image.open('images/test2.jpg')\n",
    "#image = Image.open('/common/home/users/w/wzhao/llava_helper/neutral/0a5e0d5d349fdf368afdd2b1a6e668c5899625cc4db3565ae5fe1b93340c4fc6.jpg')\n",
    "#image = Image.open(\"\")\n",
    "# Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "print(\"**************************************************************\")\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Describe this image \"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    }, \n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n",
    "print(\"**************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************\n",
      "torch.Size([1, 3, 336, 336])\n",
      "tensor(0.5515, device='cuda:0')\n",
      "USER:  \n",
      "Describe this image  ASSISTANT: The image features a large bouquet of pink roses placed in a wicker basket. The bouquet is filled with numerous pink roses, creating a beautiful and elegant display. The basket is positioned on a table, and the ros\n",
      "**************************************************************\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "image = Image.open('images/test1.png')\n",
    "#image = Image.open('/common/home/users/w/wzhao/llava_helper/neutral/0a5e0d5d349fdf368afdd2b1a6e668c5899625cc4db3565ae5fe1b93340c4fc6.jpg')\n",
    "#image = Image.open(\"\")\n",
    "# Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "print(\"**************************************************************\")\n",
    "\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"Describe this image \"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    }, \n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n",
    "print(\"**************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
